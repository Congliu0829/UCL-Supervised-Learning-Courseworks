import numpy as np
import matplotlib.pyplot as plt


def gen_data(num):
    '''
    Generate data and labels for hypothesis (random h) with given sample numbers

    Input: -num: int, number of samples

    Output: -X: array of training data in h

            -y: corresponded labels of X
    '''
    x1 = np.random.uniform(0, 1, num)
    x1 = np.expand_dims(x1, 1)
    x2 = np.random.uniform(0, 1, num)
    x2 = np.expand_dims(x2, 1)
    X = np.hstack((x1, x2))
    y = np.zeros(num)
    for i in range(num):
        y[i] = np.random.randint(2)
    return X, y


def train_test_data(num, X_h, y_h, noise=0):
    '''
    Generate training/testing data and labels with given sample numbers and random h

    Input: -num: int, number of samples

           -X_h: 100 data points generated uniform randomly.

           -y_h: corresponded 100 labels of data points 

           -noise: float in [0,1), noise when generating samples

    Output: -X: array of data 

            -y data
    '''
    x1 = np.random.uniform(0, 1, num)
    x1 = np.expand_dims(x1, 1)
    x2 = np.random.uniform(0, 1, num)
    x2 = np.expand_dims(x2, 1)
    X = np.hstack((x1, x2))
    y = np.zeros(num)
    for i in range(num):
        if np.random.binomial(1, noise):
            y[i] = np.random.randint(2)
        else:
            y[i] = knn(X_h, y_h, X[i], 3)
    return X, y


def dist(xi, X_train, _type='L2'):
    '''
    Calculate distances between a point xi and all the other points

    Input: -xi: array, one point among X data

           -X_train: array, the input data

           -_type: string, L1/L2, calculating method

    Output: a list with distances
    '''
    dist_list = []
    if _type == 'L2':
        for i in range(len(X_train)):
            dist_list.append(np.sum((xi - X_train[i])**2))

    if _type == 'L1':
        for i in range(len(X_train)):
            dist_list.append(np.sum(np.abs(xi - X_train[i])))
    return np.array(dist_list)


def predict(dist_list, y_train, k):
    '''
    Find k points with nearest distance(s)

    Input: -dist_list: array, calculated distance list

           -y_train: array, input training labels

           -k: int, number of nearest neighbors

    Output: index of k nearest points
    '''
    index_list = np.argsort(dist_list)
    label_list = []
    for i in range(k):
        label_list.append(y_train[index_list[i]])
    return np.argmax(np.bincount(label_list))


def knn(X_train, y_train, x_test, k):
    '''
    K-NN method, for a given point, first calculate all distances, then find k nearest ones

    Input: -X_train: array, training points generated by random h with noise

           -y_train: array, training labels by random h with noise

           -x_test: array, testing point

           -k: int, k nearest neighbors

    Output: list, k nearest points' index
    '''
    dist_list = dist(x_test, X_train, 'L1')
    return predict(dist_list, y_train, k)


def plot_h(X_train, y_train, k):
    '''
    Plot random h hypothesis space

    Input: -X_train: array, training points generated by random h with noise

           -y_train: array, training labels by random h with noise

           -k: int, k nearest neighbors

    Output: Plot distribution of h_sv picture
    '''
    X_test, y_test = gen_data(10000)
    label_list = []
    for x in X_test:
        label_list.append(knn(X_train, y_train, x, k))
    plt.scatter(X_test[:, 0], X_test[:, 1], c=label_list, cmap='coolwarm')
    plt.scatter(X_train[:, 0], X_train[:, 1],
                marker='x', c=y_train, cmap='plasma')
    plt.savefig('Excercise6', dpi=1000)


def label_mat(X_train, y_train, X_test):
    '''
    Find a label_matrix with size (1000, train_size)
    each row represent the top train_size nearest label

    Input: -X_train: array, training points generated by random h with noise

           -y_train: array, training labels by random h with noise

           -X_test: array testing points generated by random h with noise

    Output: a matrix with size (1000, train_size)
            each row represent the top train_size nearest label
    '''
    res = np.zeros((1000, len(y_train)), dtype=np.int64)
    for idx, x in enumerate(X_test):
        dist_list = dist(x, X_train, 'L2')
        index_list = np.argsort(dist_list)
        label_list = []
        for i in range(len(y_train)):
            label_list.append(y_train[index_list[i]])
        res[idx] = label_list
    return res


def gen_error_list(X_h, y_h, train_size, k_list):
    '''
    Input: -X_h: 100 data points generated uniform randomly.

           -y_h: corresponded 100 labels of data points

           -train_size: the size of training set

           -k_list: list, a list containing range of k that we want to use in KNN

    Output: -error_list: array, list containing generalization error list, 
                         each entry represents the generalization error 
                         averaged on 100 runs using specified k
    '''
    # same dataset with different k value
    error_list = np.zeros(len(k_list))
    for epoch in range(100):
        # 1. sample h  h = knn(X_h, y_h, x_test, 3) 100, 3
        # 2. generate 4000 training data and 1000 testing data
        X_train, y_train = train_test_data(train_size, X_h, y_h, noise=0.2)
        X_test, y_test = train_test_data(1000, X_h, y_h, noise=0.2)

        # 3. error evaluation
        label_matrix = label_mat(X_train, y_train, X_test)
        k_error_list = []

        for k in k_list:
            k_error = 0
            for i in range(len(y_test)):
                y_pred = np.argmax(np.bincount(label_matrix[i][:k]))
                if y_pred != y_test[i]:
                    k_error += 1
            k_error_list.append(k_error / len(y_test))

        error_list += np.array(k_error_list)

    error_list /= 100

    return error_list


##########################################################################################################################################################################
# Below functions are not used in excercise, but we tried to create a fast KNN algorithm
# In traditional KNN, we have to compute the distance between the test data point and each of the training data points,
# which is time-wasting, thus we come up with the idea of sampling.
# When we have the test sample point, we locate the test point within the training sample,
# then we enlarge the square range around test point, which can be regarded as sampling training samples around the specific test sample.
# when the number of training points is larger than the largest value in k_list,
# we stop the sampling process and apply knn within the sampled square. Thus we can speed up the process by calculating less distances.
# However, this method has a disadvantage. If the nearest point has not appeared before we finished the construction of k_list, then we
# may predict the label incorrectly because the iteration stops earlier. This is a method pursuing speed with compromise to the accuracy.
##########################################################################################################################################################################
# def fast_knn(X_train, y_train, x_test, k):
#     x0_test, x1_test = x_test[0], x_test[1]
#     new_X_train = []
#     new_y_train = []
#     epsilon = 0.2
#     for idx, x_train in enumerate(X_train):
#         x0_train, x1_train = x_train[0], x_train[1]
#         if len(new_X_train) >= 50: # 2 50 -> k to speed up
#             break
#         if (x0_train >= x0_test and x0_train <= x0_test + epsilon) or (x0_train <= x0_test and x0_train >= x0_test - epsilon):
#             if (x1_train >= x1_test and x1_train <= x1_test + epsilon) or (x1_train <= x1_test and x1_train >= x1_test - epsilon):
#                 new_X_train.append(x_train)
#                 new_y_train.append(y_train[idx])
#     dist_list = dist(x_test, new_X_train, 'L2')
#     return predict(dist_list, new_y_train, k)


# def fast_gen_error_list(X_h, y_h, train_size, k_list):
#     error_list = []
#     for k in k_list:
#         error = 0
#         for epoch in range(100):
#             X_train, y_train = train_test_data(train_size, X_h, y_h, noise=0.2)
#             X_test, y_test = train_test_data(1000, X_h, y_h, noise=0.2)
#             #3. error evaluation
#             one_run_error = 0
#             for i in range(len(y_test)):
#                 y_pred = fast_knn(X_train, y_train, X_test[i], k)
#                 if y_pred != y_test[i]:
#                     one_run_error += 1
#             error += one_run_error/len(y_test)
#         error_list.append(error/100)
#     return error_list
##########################################################################################################################################################################

# h distribution
X_h, y_h = gen_data(100)
plt.scatter(X_h[:, 0], X_h[:, 1], marker='o', c=y_h)
plt.show()
# visualize h distribution
plot_h(X_h, y_h, 3)
